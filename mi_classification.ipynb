{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Myocardial Infarction Classification \n",
    "\n",
    "### Introduction \n",
    "The purpose of this notebook is to design a model that classifies patients in two categories: Mycardial Infarction (MI) or Normal ECG (NORM). Considering that the dataset provides with the diagnosis likelihood, we will evaluate the impact of this variable in the model's performance. For that, we will train two models: one trained with low and high diagnosis likelihood and one trained with only high diagnosis likelihood, whereby high likelihood is defined as greater than 50%. Then, we will evaluate the model for both low and high likelihood data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization \n",
    "We start by loading the libraries needed in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import wfdb \n",
    "import ast\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm.notebook import tqdm\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import torch \n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the clinical data and options dictionnary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select sample\n",
    "with open('opts.json', 'r') as inFile:\n",
    "    opts = json.load(inFile)\n",
    "    \n",
    "# Load database\n",
    "data_df = pd.read_csv(opts['file']['ptbxl_database_processed'], index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll filter the data parsing only those subjects that have been validated by a human, to ensure clinical diagnosis, and will downsample to the category with the minimum number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select subjects that have been validated\n",
    "data_df = data_df[data_df.validated_by_human == True]\n",
    "\n",
    "# Select even sample of normal and MI subjects\n",
    "min_n = np.minimum(data_df[data_df.MI == 1].shape[0], data_df[data_df.NORM==1].shape[0])\n",
    "mi_data = data_df[data_df.MI == 1].sample(min_n)\n",
    "norm_data = data_df[data_df.NORM == 1].sample(min_n)\n",
    "data_model = pd.concat([mi_data, norm_data], axis=0)\n",
    "\n",
    "# Write \n",
    "data_model.to_csv('data_model.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've designed a Dataset class that we will use to create our Dataset and Dataloaders for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database class\n",
    "class physioDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, opts:dict, likelihood:bool=False, exclude_low_lh:bool=False, transform:transforms=None, filt:bool=False, normalize:bool=False):\n",
    "        \n",
    "        if exclude_low_lh and likelihood:\n",
    "            raise RuntimeError(\"exclude_low_lh and likelihood arguments cannot be both True\")\n",
    "        \n",
    "        self.opts = opts\n",
    "        self.likelihood = likelihood\n",
    "        self.transform = transform\n",
    "        self.filt = filt\n",
    "        self.normalize = normalize\n",
    "        self.exclude_low_lh = exclude_low_lh\n",
    "        \n",
    "        self.data_df = pd.read_csv(opts['file']['data_model'], index_col=0)\n",
    "        self.data_df_llh = self.data_df[(self.data_df.MI_lh < 50.0) | (self.data_df.NORM_lh < 50.0)]\n",
    "        \n",
    "        if exclude_low_lh:\n",
    "            self.data_df = self.data_df[~self.data_df.index.isin(self.data_df_llh.index)]\n",
    "        if likelihood:\n",
    "            self.data_df = self.data_df[(self.data_df.MI_lh < 50.0) | (self.data_df.NORM_lh < 50.0)]\n",
    "\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.data_df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        subj = self.data_df.iloc[idx]\n",
    "        signal, _ = wfdb.rdsamp(self.opts['path']['physionet'] + subj.filename_lr)\n",
    "        \n",
    "        if self.filt:\n",
    "            b, a = butter(3, [0.5, 49], fs=100, btype='band', output='ba')\n",
    "            signal = pd.DataFrame(signal).apply(lambda x: filtfilt(b,a,x)).values\n",
    "\n",
    "        if self.normalize:\n",
    "            signal = normalize(signal, axis=1)\n",
    "        \n",
    "        if self.transform:\n",
    "             signal = self.transform(signal.astype(np.float32))\n",
    "             \n",
    "        return signal, subj.MI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We design a simplified version of the [Xception](https://arxiv.org/abs/1610.02357) architecture, using simple Convolutional Layers and a smaller backbone. We've defined the loss as the Binary-Cross Entropy with Logits (which does not require a sigmoid activation in the model's classifier) and the Adam optimizer with a learning rate of 1x10<sup>-5</sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ellie(pl.LightningModule):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.entry_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=32), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.conv1_res = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=1, stride=2)\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=64), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.conv2_res = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=1, stride=2)\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=128), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Dropout2d(p=0.5)\n",
    "        )\n",
    "    \n",
    "\n",
    "        self.classifier = nn.Linear(in_features=128*125*2, out_features=n_classes)\n",
    "        \n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        self.train_accuracy = torchmetrics.Accuracy(task='binary')\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task='binary')\n",
    "        self.conf_matrix = torchmetrics.ConfusionMatrix(task='binary')\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_res = self.conv1_res(x)\n",
    "        x = self.block1(x)\n",
    "        x = torch.add(x, x_res)\n",
    "        \n",
    "        x_res = self.conv2_res(x)\n",
    "        x = self.block2(x)\n",
    "        x = torch.add(x, x_res)\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        x =  torch.flatten(x, start_dim=1)\n",
    "        x_out = self.classifier(x)\n",
    "                \n",
    "        return x_out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n",
    "        return optimizer\n",
    "    \n",
    "    def compute_step(self,batch):\n",
    "        imgs, labels = batch\n",
    "        label_logits = self.forward(imgs).flatten()\n",
    "        label_logits = torch.nan_to_num(label_logits, nan=0)\n",
    "        return self.loss(label_logits,labels.float()), labels, label_logits\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        loss, labels, label_predictions = self.compute_step(train_batch)\n",
    "        self.train_accuracy(label_predictions, labels)\n",
    "        self.log_dict({\"train/loss\": loss, 'train/acc' : self.train_accuracy}, \n",
    "                  on_step=False, \n",
    "                  on_epoch=True, \n",
    "                  prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        loss, labels, label_predictions = self.compute_step(val_batch)\n",
    "        self.val_accuracy(label_predictions, labels)\n",
    "        self.log_dict({\"val/loss\": loss, 'val/acc' : self.val_accuracy}, \n",
    "                  on_step=False, \n",
    "                  on_epoch=True, \n",
    "                  prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def test_step(self, val_batch, batch_idx):\n",
    "        loss, labels, label_predictions = self.compute_step(val_batch)\n",
    "        self.val_accuracy(label_predictions, labels)\n",
    "        self.log_dict({\"val/loss\": loss, 'val/acc' : self.val_accuracy}, \n",
    "                  on_step=False, \n",
    "                  on_epoch=True, \n",
    "                  prog_bar=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "\n",
    "We proceed to train the models. First, we create the datasets and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloadets\n",
    "torch.manual_seed(1984)\n",
    "\n",
    "# Low and high LH\n",
    "mi_data = physioDataset(opts=opts, exclude_low_lh=False, likelihood=False, transform=transforms.ToTensor(), filt=True, normalize=True)\n",
    "mi_train, mi_test = torch.utils.data.random_split(dataset=mi_data, lengths=[int(np.floor(len(mi_data)*0.8)), int(np.ceil(len(mi_data)*0.2))])\n",
    "train_dataloader, test_dataloader = DataLoader(mi_train, batch_size=32, shuffle=True, num_workers=10), DataLoader(mi_test, batch_size=32, shuffle=False, num_workers=10)\n",
    "\n",
    "# High LH\n",
    "mi_hq_data = physioDataset(opts=opts, exclude_low_lh=True, likelihood=False, transform=transforms.ToTensor(), filt=True, normalize=True)\n",
    "mi_hq_train, mi_hq_test = torch.utils.data.random_split(dataset=mi_hq_data, lengths=[int(np.floor(len(mi_hq_data)*0.8)), int(np.ceil(len(mi_hq_data)*0.2))])\n",
    "train_hq_dataloader, test_hq_dataloader = DataLoader(mi_hq_train, batch_size=32, shuffle=True, num_workers=10), DataLoader(mi_hq_test, batch_size=32, shuffle=False, num_workers=10)\n",
    "\n",
    "# Low LH (only for test)\n",
    "mi_lq_data=  physioDataset(opts=opts, exclude_low_lh=False, likelihood=True, transform=transforms.ToTensor(), filt=True, normalize=True)\n",
    "dataloader_lq = DataLoader(mi_lq_data, batch_size=32, shuffle=False, num_workers=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create two models: one that will be trained on low and high likelihood, and one trained only with high likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ellie(n_classes=1)\n",
    "model_hq = Ellie(n_classes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All data training\n",
    "First, we train the model with all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name           | Type                  | Params\n",
      "----------------------------------------------------------\n",
      "0  | entry_block    | Sequential            | 384   \n",
      "1  | block1         | Sequential            | 9.7 K \n",
      "2  | conv1_res      | Conv2d                | 64    \n",
      "3  | block2         | Sequential            | 55.7 K\n",
      "4  | conv2_res      | Conv2d                | 2.1 K \n",
      "5  | block3         | Sequential            | 221 K \n",
      "6  | classifier     | Linear                | 32.0 K\n",
      "7  | loss           | BCEWithLogitsLoss     | 0     \n",
      "8  | train_accuracy | BinaryAccuracy        | 0     \n",
      "9  | val_accuracy   | BinaryAccuracy        | 0     \n",
      "10 | conf_matrix    | BinaryConfusionMatrix | 0     \n",
      "----------------------------------------------------------\n",
      "321 K     Trainable params\n",
      "0         Non-trainable params\n",
      "321 K     Total params\n",
      "1.288     Total estimated model params size (MB)\n",
      "/home/vicente/Documents/Idoven-Data-Scientist/env/lib/python3.8/site-packages/pytorch_lightning/loggers/tensorboard.py:261: UserWarning: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 219/219 [00:20<00:00, 10.78it/s, loss=0.254, v_num=2, val/loss=0.253, val/acc=0.890, train/loss=0.242, train/acc=0.905]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 219/219 [00:20<00:00, 10.76it/s, loss=0.254, v_num=2, val/loss=0.253, val/acc=0.890, train/loss=0.242, train/acc=0.905]\n"
     ]
    }
   ],
   "source": [
    "# Train model in all data\n",
    "tb_logger = pl_loggers.TensorBoardLogger(\"/home/vicente/Documents/Idoven-Data-Scientist/logs\", log_graph=True)\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=10, auto_lr_find=False, logger=tb_logger, callbacks=[TQDMProgressBar(refresh_rate=10)])\n",
    "trainer.fit(model, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We empty the GPU cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the performance. Training and validation curves seem normal, with a final validation accuracy of ~89%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6013 (pid 55958), started 0:00:03 ago. (Use '!kill 55958' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-817150adc6d903ce\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-817150adc6d903ce\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6013;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard \n",
    "%tensorboard --logdir 'logs/lightning_logs/version_2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### High likelihood data training\n",
    "We now train the model with only high-likelihood data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vicente/Documents/Idoven-Data-Scientist/env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name           | Type                  | Params\n",
      "----------------------------------------------------------\n",
      "0  | entry_block    | Sequential            | 384   \n",
      "1  | block1         | Sequential            | 9.7 K \n",
      "2  | conv1_res      | Conv2d                | 64    \n",
      "3  | block2         | Sequential            | 55.7 K\n",
      "4  | conv2_res      | Conv2d                | 2.1 K \n",
      "5  | block3         | Sequential            | 221 K \n",
      "6  | classifier     | Linear                | 32.0 K\n",
      "7  | loss           | BCEWithLogitsLoss     | 0     \n",
      "8  | train_accuracy | BinaryAccuracy        | 0     \n",
      "9  | val_accuracy   | BinaryAccuracy        | 0     \n",
      "10 | conf_matrix    | BinaryConfusionMatrix | 0     \n",
      "----------------------------------------------------------\n",
      "321 K     Trainable params\n",
      "0         Non-trainable params\n",
      "321 K     Total params\n",
      "1.288     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 187/187 [00:20<00:00,  9.33it/s, loss=0.158, v_num=1, val/loss=0.194, val/acc=0.922, train/loss=0.172, train/acc=0.932]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 187/187 [00:20<00:00,  9.30it/s, loss=0.158, v_num=1, val/loss=0.194, val/acc=0.922, train/loss=0.172, train/acc=0.932]\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "tb_logger_hq = pl_loggers.TensorBoardLogger(\"/home/vicente/Documents/Idoven-Data-Scientist/logs_hq\")\n",
    "trainer_hq = pl.Trainer(gpus=1,max_epochs=10, auto_lr_find=False, logger=tb_logger_hq)\n",
    "trainer_hq.fit(model_hq, train_hq_dataloader, test_hq_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clear the GPU cache again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model's training routine. Learning curves seem fine, with a final validation accuracy of ~92%, marginally higher than the model trained with low likelihood as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6014 (pid 59431), started 0:00:02 ago. (Use '!kill 59431' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e4d02eb151f4a538\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e4d02eb151f4a538\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6014;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard \n",
    "%tensorboard --logdir 'logs_hq/lightning_logs/version_1/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of low and high likelihood.\n",
    "\n",
    "We want to check how each model did, and compare the differences in performance between the high and low likelihood. We begin with the first model. Let's look first at the test data used to evaluate the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/vicente/Documents/Idoven-Data-Scientist/env/lib/python3.8/site-packages/pytorch_lightning/loggers/tensorboard.py:261: UserWarning: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 44/44 [00:02<00:00, 17.40it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         val/acc            0.8901650905609131\n",
      "        val/loss            0.2525832951068878\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val/loss': 0.2525832951068878, 'val/acc': 0.8901650905609131}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eval on low HQ\n",
    "trainer.test(model=model, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how it does with only likelihood greater than 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/vicente/Documents/Idoven-Data-Scientist/env/lib/python3.8/site-packages/pytorch_lightning/loggers/tensorboard.py:261: UserWarning: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 1000it [31:01,  1.86s/it] \n",
      "Testing: 30it [00:03,  9.42it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         val/acc            0.9437447786331177\n",
      "        val/loss            0.15542574226856232\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val/loss': 0.15542574226856232, 'val/acc': 0.9437447786331177}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=model, dataloaders=test_hq_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It performs marginally better, with and increase of ~5% and crossing the 90% accuracy boundary. We proceed to check the low likelihood data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/vicente/Documents/Idoven-Data-Scientist/env/lib/python3.8/site-packages/pytorch_lightning/loggers/tensorboard.py:261: UserWarning: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 32/32 [00:01<00:00, 21.82it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         val/acc            0.7318768501281738\n",
      "        val/loss             0.537009060382843\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val/loss': 0.537009060382843, 'val/acc': 0.7318768501281738}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=model, dataloaders=dataloader_lq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's accuracy is dragged down by the low likelihood data, despite being trained with part of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at how the model trained with high-likelihood data did. We begin to evaluate the performance of the validation set used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 38/38 [00:01<00:00, 19.27it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         val/acc            0.9219143390655518\n",
      "        val/loss            0.1935616284608841\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val/loss': 0.1935616284608841, 'val/acc': 0.9219143390655518}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_hq.test(model=model_hq, dataloaders=test_hq_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's evaluate the model on low-likelihood data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 32/32 [00:01<00:00, 18.95it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         val/acc            0.7507447600364685\n",
      "        val/loss            0.5656914710998535\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val/loss': 0.5656914710998535, 'val/acc': 0.7507447600364685}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_hq.test(model=model_hq, dataloaders=dataloader_lq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's performance is severely affected by the low-likelihood data, with a decrease in accuracy of ~20%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "The purpose of the noteboook was to evaluate the impact of likelihood in a classification model's performance. For this purpose, we've trained two models with low and high diagnostic likelihood, and high diagnostic likelihood exclusively. After assessing the classification accuracy of both models, we can conclude that low-likelihood decreases accuracy in about 20% regardless of whether the model was trained with low diagnostic likelihood, or not. \n",
    "\n",
    "Diagnotic likelihood is rarely included in medical dataset despite being a reality for practioners. Doctors cannot always ensure a diagnosis. This diagnosis variability influences model performance, and adds noise to the data. Therefore, it's a relevant variable worth requesting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a8e3449f4926d277380905eec2b276a450e0f810ff6569caca100977063163a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
